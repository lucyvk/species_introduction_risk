{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import niche\n",
    "import metrics\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "out_location = \"paper_webs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intro1_metric - final biomass of first introduced species\n",
    "# intro2_metric - final biomass of second introduced species\n",
    "# es_metric - change in ecosystem service amount\n",
    "# bio_metric - fraction resident species remaining\n",
    "# es_raw - raw final ecosystem service amount (used in trade-off plots)\n",
    "metric_header = [\"intro1_metric\",\"intro2_metric\",\"es_metric\",\"bio_metric\",\"es_raw\"]\n",
    "management_ops = [\"ct\",\"inv1\",\"inv2\",\"invb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final metrics for plotting and save to file \n",
    "\n",
    "fif_webs_file = open('./' + out_location + '/over15_webs.txt','r')\n",
    "\n",
    "# counterfactual metric results (only 1 per web, so all listed in the same file)\n",
    "ct_file = open('./' + out_location + '/counterfactual_results_newmet.csv','w')\n",
    "ct_writer = csv.writer(ct_file,dialect=\"excel\")\n",
    "ct_writer.writerow([\"id\"] + metric_header)\n",
    "\n",
    "# results for if the ecosystem service is provided by one basal species\n",
    "es_res_files = {}\n",
    "es_res_writers = {}\n",
    "es_rsd_files = {}\n",
    "es_rsd_writers = {}\n",
    "for m in management_ops:\n",
    "    es_res_files[m] = open('./' + out_location + f'/one_es_res_{m}.csv','w')\n",
    "    es_res_writers[m] = csv.writer(es_res_files[m])\n",
    "    if m != \"ct\":\n",
    "        es_rsd_files[m] = open('./' + out_location + f'/one_es_res_{m}_rsd.csv','w')\n",
    "        es_rsd_writers[m] = csv.writer(es_rsd_files[m])\n",
    "for m in es_res_writers:\n",
    "    if m != \"ct\":\n",
    "        es_res_writers[m].writerow([\"web_id\",\"es_node_id\",\"es_change\",\"es_raw\"])\n",
    "    else:\n",
    "        es_res_writers[m].writerow([\"web_id\",\"es_node_id\",\"es_mean_change\",\"es_med_change\",\"es_mean_raw\",\"es_med_raw\"])\n",
    "for m in es_rsd_writers:\n",
    "    es_rsd_writers[m].writerow([\"web_id\",\"one_es_rsd\"])\n",
    "\n",
    "for row in fif_webs_file:\n",
    "    i = int(row)\n",
    "    \n",
    "    # initial biomass vector for web (before introduction, after run to equilibrium)\n",
    "    webi,_,_,_,b_init_web = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_2000\")\n",
    "        \n",
    "    # save metrics for counterfactual dynamics (4000 total timesteps with no introduction)\n",
    "    webf,_,_,_,b_final_ct = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_2000_2000\")\n",
    "    intro1_ct, intro2_ct, es_ct, bio_ct, es_raw = metrics.get_metrics(webi,webf,b_init_web,b_final_ct,None,None,metrics.es_linear)\n",
    "    ct_writer.writerow([i, intro1_ct, intro2_ct, es_ct, bio_ct, es_raw])\n",
    "    one_es_res = []\n",
    "    one_es_res_raw = []\n",
    "    for node in niche.get_basal_ids(webi):\n",
    "        ges = metrics.get_es_spec(b_init_web,b_final_ct,node)\n",
    "        one_es_res.append(ges[0])\n",
    "        one_es_res_raw.append(ges[1])\n",
    "        es_res_writers['ct'].writerow([i,node,ges[0],ges[1]])\n",
    "            \n",
    "    # first introduced species\n",
    "    with open('./' + out_location + '/introduced_1.csv','r') as inv_1_info:\n",
    "        inv1_reader = csv.reader(inv_1_info)\n",
    "        next(inv1_reader)\n",
    "        inv_1_res_dict = {}\n",
    "        for line in inv1_reader:\n",
    "            inv_id = int(line[0])\n",
    "            # invaded web with additional 2000 timesteps after invasion\n",
    "            webf,_,_,_,b_final_1 = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/one/web_\" + str(i) + \\\n",
    "                                                         \"_inv_\" + str(inv_id) + \"_2000\")\n",
    "            intro1_inv, intro2_inv, es_inv, bio_inv, es_raw = metrics.get_metrics(webi,webf,b_init_web,b_final_1,inv_id,None,metrics.es_linear)\n",
    "            inv_1_res_dict[inv_id] = [intro1_inv, intro2_inv, es_inv, bio_inv, es_raw]\n",
    "            for_rsd, es_nodes, es_nodes_raw = metrics.get_metrics_single_es(b_init_web, b_final_1, webi)                      \n",
    "    with open('./' + out_location + '/web_' + str(i) + '_introduced_1_newmet.csv','w') as inv_1_res:\n",
    "        inv1_writer = csv.writer(inv_1_res,dialect=\"excel\")\n",
    "        inv1_writer.writerow([\"inv1_id\"] + metric_header)\n",
    "        for key in inv_1_res_dict:\n",
    "            inv1_writer.writerow([key] + inv_1_res_dict[key])\n",
    "    for node in niche.get_basal_ids(webi):        \n",
    "        es_res_writers['inv1'].writerow([i,node,np.mean(es_nodes[node]),np.median(es_nodes[node]),np.mean(es_nodes_raw[node]),np.median(es_nodes_raw[node])])\n",
    "    es_rsd_writers['inv1'].writerow([i,metrics.get_rsd(for_rsd)])\n",
    "\n",
    "    # second introduced species\n",
    "    with open('./' + out_location + '/introduced_2.csv','r') as inv_2_info:\n",
    "        inv2_reader = csv.reader(inv_2_info)\n",
    "        next(inv2_reader)\n",
    "        inv_2_res_dict = {}\n",
    "        for line in inv2_reader:\n",
    "            inv_id = int(line[0])\n",
    "            webf,_,_,_,b_final_2 = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/two/web_\" + str(i) + \\\n",
    "                                                         \"_inv_\" + str(inv_id) + \"_2000\")\n",
    "            intro1_inv, intro2_inv, es_inv, bio_inv, es_raw = metrics.get_metrics(webi,webf,b_init_web,b_final_2,None,inv_id,metrics.es_linear)\n",
    "            inv_2_res_dict[inv_id] = [intro1_inv, intro2_inv, es_inv, bio_inv, es_raw]\n",
    "            for_rsd, es_nodes, es_nodes_raw = metrics.get_metrics_single_es(b_init_web, b_final_2, webi)                         \n",
    "    with open('./' + out_location + '/web_' + str(i) + '_introduced_2_newmet.csv','w') as inv_2_res:\n",
    "        inv2_writer = csv.writer(inv_2_res,dialect=\"excel\")\n",
    "        inv2_writer.writerow([\"inv2_id\"] + metric_header)\n",
    "        for key in inv_2_res_dict:\n",
    "            inv2_writer.writerow([key] + inv_2_res_dict[key])\n",
    "        inv_2_res.close()\n",
    "    es_rsd_writers['inv2'].writerow([i,metrics.get_rsd(for_rsd)])\n",
    "    for node in niche.get_basal_ids(webi):\n",
    "        es_res_writers['inv2'].writerow([i,node,np.mean(es_nodes[node]),np.median(es_nodes[node]),np.mean(es_nodes_raw[node]),np.median(es_nodes_raw[node])])\n",
    "    \n",
    "    # both introduced species\n",
    "    with open('./' + out_location + '/introduced_both.csv','r') as inv_both_info:\n",
    "        inv_both_reader = csv.reader(inv_both_info)\n",
    "        next(inv_both_reader)\n",
    "        inv_both_res_dict = {}\n",
    "        for line in inv_both_reader:\n",
    "            inv1_id = int(line[0])\n",
    "            inv2_id = int(line[1])\n",
    "            webf,_,_,_,b_final_b = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/both/web_\" + str(i) + \"_inv_\" + str(inv1_id) + '_' + str(inv2_id) + \"_2000\")\n",
    "            intro1_inv, intro2_inv, es_inv, bio_inv, es_raw = metrics.get_metrics(webi,webf,b_init_web,b_final_b,inv1_id,inv2_id,metrics.es_linear)\n",
    "            inv_both_res_dict[(inv1_id,inv2_id)] = [intro1_inv, intro2_inv, es_inv, bio_inv, es_raw]\n",
    "            for_rsd, es_nodes, es_nodes_raw = metrics.get_metrics_single_es(b_init_web, b_final_b, webi)    \n",
    "    with open('./' + out_location + '/web_' + str(i) + '_introduced_both_newmet.csv','w') as inv_both_res:\n",
    "        inv_both_writer = csv.writer(inv_both_res,dialect=\"excel\")\n",
    "        inv_both_writer.writerow([\"inv1_id\",\"inv2_id\"] + metric_header)\n",
    "        for key in inv_both_res_dict:\n",
    "            inv_both_writer.writerow([key[0],key[1]] + inv_both_res_dict[key])\n",
    "    es_rsd_writers['invb'].writerow([i,metrics.get_rsd(for_rsd)])\n",
    "    for node in niche.get_basal_ids(webi):\n",
    "        es_res_writers['invb'].writerow([i,node,np.mean(es_nodes[node]),np.median(es_nodes[node]),np.mean(es_nodes_raw[node]),np.median(es_nodes_raw[node])])\n",
    "\n",
    "fif_webs_file.close()\n",
    "ct_file.close()\n",
    "for m in es_res_files:\n",
    "    es_res_files[m].close()\n",
    "for m in es_rsd_files:\n",
    "    es_rsd_files[m].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
