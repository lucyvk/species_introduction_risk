{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luva6824/jhub_venv/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import niche\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "out_location = \"paper_webs_0511\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final metrics for plotting and save\n",
    "\n",
    "def get_intermediate_ids(net):\n",
    "    int_species = []\n",
    "    for node in net.nodes():\n",
    "        \n",
    "        #top predator would have no outgoing edges\n",
    "        out_neighs = []\n",
    "        out_neigh_list = list(net.out_edges([node]))\n",
    "        for out_neigh in out_neigh_list:\n",
    "            out_neighs.append(out_neigh[1])\n",
    "            \n",
    "        in_neighs = []\n",
    "        in_neigh_list = list(net.in_edges([node]))\n",
    "        for in_neigh in in_neigh_list: #curr_web.in_degree(node):\n",
    "            in_neighs.append(in_neigh[0])\n",
    "            \n",
    "        if len(out_neighs) > 0 and len(in_neighs) > 0:\n",
    "            int_species.append(node)\n",
    "            \n",
    "    return int_species\n",
    "\n",
    "def get_basal_ids(net):\n",
    "    basal_species = []\n",
    "    \n",
    "    for node in net.nodes():\n",
    "        \n",
    "        out_neighs = []\n",
    "        out_neigh_list = list(net.out_edges([node]))\n",
    "        for out_neigh in out_neigh_list:\n",
    "            out_neighs.append(out_neigh[1])\n",
    "            \n",
    "        in_neighs = []\n",
    "        in_neigh_list = list(net.in_edges([node]))\n",
    "        for in_neigh in in_neigh_list: #curr_web.in_degree(node):\n",
    "            in_neighs.append(in_neigh[0])\n",
    "            \n",
    "        if len(out_neighs) > 0 and len(in_neighs) == 0:\n",
    "            basal_species.append(node)\n",
    "            \n",
    "    return basal_species\n",
    "    \n",
    "    \n",
    "# four metrics of interest returned:\n",
    "# intro_metric1 - final biomass of introduced species 1\n",
    "# intro_metric2 - final biomass of introduced species 2\n",
    "# es_metric - % change in es amount\n",
    "# bio_metric - fraction resident species remaining\n",
    "def get_metrics(web_init, web_final, b_init,b_final,inv_id1,inv_id2,es_fn):\n",
    "        \n",
    "    #based off of aggregate of intermediate species biomasses\n",
    "    #init_int = get_intermediate_ids(web_init)\n",
    "    init_int = get_basal_ids(web_init)\n",
    "    b_total_init = 0\n",
    "    for node in init_int:\n",
    "        b_total_init += b_init[node]\n",
    "    \n",
    "    initial_es = es_fn(b_total_init)\n",
    "    initial_bio = len(b_init.keys())\n",
    "    \n",
    "    inv_adj = 0\n",
    "    intro_metric1 = 0\n",
    "    intro_metric2 = 0\n",
    "    if inv_id1 and inv_id1 in b_final:\n",
    "        intro_metric1 = b_final[inv_id1]\n",
    "        inv_adj = inv_adj - 1\n",
    "    if inv_id2 and inv_id2 in b_final:\n",
    "        intro_metric2 = b_final[inv_id2]\n",
    "        inv_adj = inv_adj - 1\n",
    "    \n",
    "    #final_int = get_intermediate_ids(web_final)\n",
    "    final_int = get_basal_ids(web_final)\n",
    "    b_total_final = 0\n",
    "    for node in final_int:\n",
    "        if node != inv_id1 and node != inv_id2:\n",
    "            b_total_final += b_final[node]\n",
    "    \n",
    "    final_es = es_fn(b_total_final)\n",
    "    final_bio = len(b_final.keys()) + inv_adj # subtract out the invader counts\n",
    "    \n",
    "    es_metric = (final_es - initial_es)/initial_es*100\n",
    "    bio_metric = final_bio/initial_bio\n",
    "    \n",
    "    return intro_metric1, intro_metric2, es_metric, bio_metric\n",
    "    \n",
    "# get es metric assuming it's provided by one species\n",
    "def get_es_spec(b_init, b_final, es_spec):\n",
    "    \n",
    "    initial_es = es_linear(b_init[es_spec])\n",
    "    \n",
    "    if es_spec in b_final:\n",
    "        final_es = es_linear(b_final[es_spec])\n",
    "    else:\n",
    "        final_es = 0    \n",
    "    \n",
    "    es_metric = (final_es - initial_es)/initial_es*100\n",
    "    \n",
    "    return es_metric\n",
    "    \n",
    "#maps the value of species biomass to es value\n",
    "def es_linear(b_val):\n",
    "    return b_val*0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final metrics for plotting and save to file \n",
    "\n",
    "fif_webs_file = open('./' + out_location + '/over15_webs.txt','r')\n",
    "ct_file = open('./' + out_location + '/counterfactual_results_newmet.csv','w')\n",
    "ct_writer = csv.writer(ct_file,dialect=\"excel\")\n",
    "ct_writer.writerow([\"web_id\",\"intro1_metric\",\"Ã¯ntro2_metric\",\"es_metric\",\"bio_metric\"])\n",
    "\n",
    "es_file1 = open('./' + out_location + '/all_es_res_ct.csv','w')\n",
    "esw_ct = csv.writer(es_file1)\n",
    "esw_ct.writerow([\"web_id\",\"node_id\",\"es_metric\"])\n",
    "es_file2 = open('./' + out_location + '/all_es_res_inv1.csv','w')\n",
    "esw_inv1 = csv.writer(es_file2)\n",
    "esw_inv1.writerow([\"web_id\",\"node_id\",\"es_metric\"])\n",
    "es_file3 = open('./' + out_location + '/all_es_res_inv2.csv','w')\n",
    "esw_inv2 = csv.writer(es_file3)\n",
    "esw_inv2.writerow([\"web_id\",\"node_id\",\"es_metric\"])\n",
    "es_file4 = open('./' + out_location + '/all_es_res_invb.csv','w')\n",
    "esw_invb = csv.writer(es_file4)\n",
    "esw_invb.writerow([\"web_id\",\"node_id\",\"es_metric\"])\n",
    "\n",
    "for row in fif_webs_file:\n",
    "    i = int(row)\n",
    "    \n",
    "    #initial biomass vector for web (before introduction, after run to equilibrium)\n",
    "    webi,_,_,_,b_init_web = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_2000\")\n",
    "        \n",
    "    # save metrics for counterfactual dynamics (4000 total timesteps with no introduction)\n",
    "    webf,_,_,_,b_final_ct = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_2000_2000\")\n",
    "    intro1_ct, intro2_ct, es_ct, bio_ct = get_metrics(webi,webf,b_init_web,b_final_ct,None,None,es_linear)\n",
    "    \n",
    "    ct_writer.writerow([i, intro1_ct, intro2_ct, es_ct, bio_ct])\n",
    "    for node in get_basal_ids(webi):\n",
    "        esw_ct.writerow([i,node,get_es_spec(b_init_web,b_final_ct,node)])\n",
    "        \n",
    "    # invader 1\n",
    "    inv_1_info = open('./' + out_location + '/invader_1.csv','r')\n",
    "    inv1_reader = csv.reader(inv_1_info)\n",
    "    next(inv1_reader)\n",
    "    #object for temporary saving\n",
    "    es_nodes = {}\n",
    "    inv_1_res_dict = {}\n",
    "    for line in inv1_reader:\n",
    "        inv_id = int(line[0])\n",
    "        #invaded web with additional 2000 timesteps after invasion\n",
    "        webf,_,_,_,b_final_1 = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/one/web_\" + str(i) + \\\n",
    "                                                     \"_inv_\" + str(inv_id) + \"_2000\")\n",
    "        intro1_inv, intro2_inv, es_inv, bio_inv = get_metrics(webi,webf,b_init_web,b_final_1,inv_id,None,es_linear)\n",
    "        inv_1_res_dict[inv_id] = [intro1_inv, intro2_inv, es_inv, bio_inv]\n",
    "        \n",
    "        for node in webi.nodes():\n",
    "            if node not in es_nodes:\n",
    "                es_nodes[node] = [get_es_spec(b_init_web,b_final_1,node)]\n",
    "            else:\n",
    "                es_nodes[node].append(get_es_spec(b_init_web,b_final_1,node))\n",
    "            \n",
    "    inv_1_info.close()\n",
    "\n",
    "    #write invader 1 results to file \n",
    "    inv_1_res = open('./' + out_location + '/web_' + str(i) + '_invader_1_newmet.csv','w')\n",
    "    inv1_writer = csv.writer(inv_1_res,dialect=\"excel\")\n",
    "    inv1_writer.writerow(['inv_id','intro1_metric','intro2_metric','es_metric','bio_metric'])\n",
    "    for key in inv_1_res_dict:\n",
    "        inv1_writer.writerow([key] + inv_1_res_dict[key])\n",
    "    inv_1_res.close()\n",
    "    \n",
    "    for node in get_basal_ids(webi):\n",
    "        esw_inv1.writerow([i,node,np.mean(es_nodes[node])])\n",
    "\n",
    "    # invader 2\n",
    "    inv_2_info = open('./' + out_location + '/invader_2.csv','r')\n",
    "    inv2_reader = csv.reader(inv_2_info)\n",
    "    next(inv2_reader)\n",
    "    #object for temporary saving\n",
    "    inv_2_res_dict = {}\n",
    "    es_nodes = {}\n",
    "    for line in inv2_reader:\n",
    "        inv_id = int(line[0])\n",
    "        #invaded web with additional 2000 timesteps after invasion\n",
    "        webf,_,_,_,b_final_2 = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/two/web_\" + str(i) + \\\n",
    "                                                     \"_inv_\" + str(inv_id) + \"_2000\")\n",
    "        intro1_inv, intro2_inv, es_inv, bio_inv = get_metrics(webi,webf,b_init_web,b_final_2,None,inv_id,es_linear)\n",
    "        inv_2_res_dict[inv_id] = [intro1_inv, intro2_inv, es_inv, bio_inv]\n",
    "        \n",
    "        for node in webi.nodes():\n",
    "            if node not in es_nodes:\n",
    "                es_nodes[node] = [get_es_spec(b_init_web,b_final_2,node)]\n",
    "            else:\n",
    "                es_nodes[node].append(get_es_spec(b_init_web,b_final_2,node))\n",
    "                \n",
    "    inv_2_info.close()\n",
    "\n",
    "    #write invader 2 results to file \n",
    "    inv_2_res = open('./' + out_location + '/web_' + str(i) + '_invader_2_newmet.csv','w')\n",
    "    inv2_writer = csv.writer(inv_2_res,dialect=\"excel\")\n",
    "    inv2_writer.writerow(['inv_id','intro1_metric','intro2_metric','es_metric','bio_metric'])\n",
    "    for key in inv_2_res_dict:\n",
    "        inv2_writer.writerow([key] + inv_2_res_dict[key])\n",
    "    inv_2_res.close()\n",
    "    \n",
    "    for node in get_basal_ids(webi):\n",
    "        esw_inv2.writerow([i,node,np.mean(es_nodes[node])])\n",
    "    \n",
    "    # both invaders \n",
    "    inv_both_info = open('./' + out_location + '/invader_both.csv','r')\n",
    "    inv_both_reader = csv.reader(inv_both_info)\n",
    "    next(inv_both_reader)\n",
    "    #object for temporary saving\n",
    "    inv_both_res_dict = {}\n",
    "    es_nodes = {}\n",
    "    for line in inv_both_reader:\n",
    "        inv1_id = int(line[0])\n",
    "        inv2_id = int(line[1])\n",
    "        #invaded web with additional 2000 timesteps after invasion\n",
    "        webf,_,_,_,b_final_b = niche.read_web_from_file(\"./\" + out_location + \"/web_\" + str(i) + \"_inv/both/web_\" + str(i) + \"_inv_\" + str(inv1_id) + '_' + str(inv2_id) + \"_2000\")\n",
    "        intro1_inv, intro2_inv, es_inv, bio_inv = get_metrics(webi,webf,b_init_web,b_final_b,inv1_id,inv2_id,es_linear)\n",
    "        inv_both_res_dict[(inv1_id,inv2_id)] = [intro1_inv, intro2_inv, es_inv, bio_inv]\n",
    "        \n",
    "        for node in webi.nodes():\n",
    "            if node not in es_nodes:\n",
    "                es_nodes[node] = [get_es_spec(b_init_web,b_final_b,node)]\n",
    "            else:\n",
    "                es_nodes[node].append(get_es_spec(b_init_web,b_final_b,node))\n",
    "        \n",
    "    inv_both_info.close()\n",
    "\n",
    "    #write both invader results to file \n",
    "    inv_both_res = open('./' + out_location + '/web_' + str(i) + '_invader_both_newmet.csv','w')\n",
    "    inv_both_writer = csv.writer(inv_both_res,dialect=\"excel\")\n",
    "    inv_both_writer.writerow(['inv_id1','inv_id2','intro1_metric','intro2_metric','es_metric','bio_metric'])\n",
    "    for key in inv_both_res_dict:\n",
    "        inv_both_writer.writerow([key[0],key[1]] + inv_both_res_dict[key])\n",
    "    inv_both_res.close()\n",
    "    \n",
    "    for node in get_basal_ids(webi):\n",
    "        esw_invb.writerow([i,node,np.mean(es_nodes[node])])\n",
    "\n",
    "fif_webs_file.close()\n",
    "ct_file.close()\n",
    "es_file1.close()\n",
    "es_file2.close()\n",
    "es_file3.close()\n",
    "es_file4.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
